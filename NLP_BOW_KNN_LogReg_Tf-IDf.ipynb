{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/preethamvignesh/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/preethamvignesh/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/preethamvignesh/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Error loading stop: Package 'stop' not found in index\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/preethamvignesh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stop')\n",
    "nltk.download('stopwords')\n",
    "# nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdata1 = pd.read_csv(\"~/Desktop/Work/ML_EIT/Data/tweets_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1863 entries, 0 to 1862\n",
      "Data columns (total 2 columns):\n",
      " #   Column                               Non-Null Count  Dtype  \n",
      "---  ------                               --------------  -----  \n",
      " 0   *screams in 25 different languages*  1863 non-null   object \n",
      " 1   0.6                                  1863 non-null   float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 29.2+ KB\n"
     ]
    }
   ],
   "source": [
    "tdata1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1863, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdata1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Families to sue over Legionnaires: More than 4...\n",
       "1       Pandemonium In Aba As Woman Delivers Baby With...\n",
       "2       My emotions are a train wreck. My body is a tr...\n",
       "3       Alton brown just did a livestream and he burne...\n",
       "4       @TinyJecht Are you another Stand-user? If you ...\n",
       "                              ...                        \n",
       "1858    @Trollkrattos Juan Carlos Salvador The Secret ...\n",
       "1859    @devon_breneman hopefully it doesn't electrocu...\n",
       "1860    Businesses are deluged with invokces. Make you...\n",
       "1861    #BREAKING411 4 police officers arrested for ab...\n",
       "1862    @News@ Refugio oil spill may have been costlie...\n",
       "Name: tweet, Length: 1863, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# renamming the column names for the text dataset\n",
    "tdata = tdata1.rename(columns={\"*screams in 25 different languages*\":\"tweet\", \"0.6\":\"lble\"})\n",
    "tdata.tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Families to sue over Legionnaires: More than 40 families affected by the fatal outbreak of Legionnaires' disea... http://t.co/ZA4AXFJSVB\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdata.tweet[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>lble</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Families to sue over Legionnaires: More than 4...</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pandemonium In Aba As Woman Delivers Baby With...</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>My emotions are a train wreck. My body is a tr...</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alton brown just did a livestream and he burne...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@TinyJecht Are you another Stand-user? If you ...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  lble  label\n",
       "0  Families to sue over Legionnaires: More than 4...   0.1      1\n",
       "1  Pandemonium In Aba As Woman Delivers Baby With...   0.4      2\n",
       "2  My emotions are a train wreck. My body is a tr...   0.2      1\n",
       "3  Alton brown just did a livestream and he burne...   0.5      3\n",
       "4  @TinyJecht Are you another Stand-user? If you ...   0.5      3"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Binning the y values to integers\n",
    "bins = np.array([0.0,0.25,0.5,0.75])\n",
    "# bins = np.array([0.0,0.2,0.4,0.6,0.8])\n",
    "tdata['label'] = np.digitize(tdata['lble'],bins)\n",
    "# tdata['label'] = np.digitize(tdata['lble'],bins=[0.5])\n",
    "tdata.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocessing text file, which are neededed for transferring text\n",
    "# from human language to machine language format for further processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1863,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ptext = tdata.tweet\n",
    "ydata = tdata.label\n",
    "ptext.shape\n",
    "\n",
    "# import math \n",
    "# nn = math.ceil(len(tdata.tweet)*0.7)\n",
    "# print(nn)\n",
    "\n",
    "# Xtrain = tdata.tweet[:nn]\n",
    "# Xtest= tdata.tweet[nn:]\n",
    "# y_train = (10*tdata.lble[:nn]).to_numpy(dtype='int64')\n",
    "# y_test = (10*tdata.lble[nn:]).to_numpy(dtype='int64')\n",
    "# # prices1.to_numpy(dtype='int64')\n",
    "\n",
    "# print(Xtrain.shape, Xtest.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Families to sue over Legionnaires: More than 40 families affected by the fatal outbreak of Legionnaires' disea... http://t.co/ZA4AXFJSVB\n"
     ]
    }
   ],
   "source": [
    "print(ptext[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "# temp = []\n",
    "snow = nltk.stem.SnowballStemmer('english')\n",
    "def preprocess(data):\n",
    "    temp = []\n",
    "    for sentence in data:\n",
    "        sentence = sentence.lower()     #converting lowercase\n",
    "\n",
    "        #Removal of HTTP links/URLs mixed up in any text:\n",
    "        sentence = re.sub('http://\\S+|https://\\S+', '', sentence)\n",
    "        #OR \n",
    "        #sentence = re.sub('http[s]?://\\S+', '', sentence)\n",
    "        #sentence = re.sub(r'https?:\\/\\/.*[\\r\\n]*','',sentence, flags=re.MULTILINE) #remove the URLs\n",
    "\n",
    "        #punctuations:'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
    "        sentence = \"\".join([char for char in sentence if char not in string.punctuation])\n",
    "        sentence = re.sub(r'\\d+', '', sentence)    # removing numbers\n",
    "        sentence = sentence.strip()        # removing extra white spaces\n",
    "        tokens = nltk.word_tokenize(sentence)[2 :]\n",
    "    #     tokens = nltk.word_tokenize(sentence)\n",
    "        sentence = [l.lower() for l in tokens]\n",
    "        filtered_result = list(filter(lambda l: l not in stop_words, sentence))\n",
    "        lemmas = [wordnet_lemmatizer.lemmatize(t) for t in filtered_result]    \n",
    "\n",
    "        #(or)\n",
    "    #     sentence = re.sub(r'[?|!|\\'|\"|#!$%&()*+-@]',r'',sentence)\n",
    "    #     sentence = re.sub(r'[.|,|)|(|\\|/]',r' ',sentence)\n",
    "        \n",
    "        temp.append(lemmas) \n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xdata_cleaned = preprocess(ptext)\n",
    "# Xtest_cleaned = preprocess(Xtest)\n",
    "ptext_cleaned = preprocess(ptext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['sue',\n",
       "  'legionnaire',\n",
       "  'family',\n",
       "  'affected',\n",
       "  'fatal',\n",
       "  'outbreak',\n",
       "  'legionnaire',\n",
       "  'disea'],\n",
       " ['aba', 'woman', 'delivers', 'baby', 'without', 'face', 'photo']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(Xdata_cleaned[0])\n",
    "ptext_cleaned[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1863, 4)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalized or cleaning dataset adding into original datafile for chcking\n",
    "tdata['Normalized_tweet'] = ptext_cleaned\n",
    "tdata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>Normalized_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Families to sue over Legionnaires: More than 4...</td>\n",
       "      <td>[sue, legionnaire, family, affected, fatal, ou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pandemonium In Aba As Woman Delivers Baby With...</td>\n",
       "      <td>[aba, woman, delivers, baby, without, face, ph...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>My emotions are a train wreck. My body is a tr...</td>\n",
       "      <td>[train, wreck, body, train, wreck, im, wreck]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alton brown just did a livestream and he burne...</td>\n",
       "      <td>[livestream, burned, butter, touched, hot, pla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@TinyJecht Are you another Stand-user? If you ...</td>\n",
       "      <td>[another, standuser, detonate, killer, queen]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  \\\n",
       "0  Families to sue over Legionnaires: More than 4...   \n",
       "1  Pandemonium In Aba As Woman Delivers Baby With...   \n",
       "2  My emotions are a train wreck. My body is a tr...   \n",
       "3  Alton brown just did a livestream and he burne...   \n",
       "4  @TinyJecht Are you another Stand-user? If you ...   \n",
       "\n",
       "                                    Normalized_tweet  \n",
       "0  [sue, legionnaire, family, affected, fatal, ou...  \n",
       "1  [aba, woman, delivers, baby, without, face, ph...  \n",
       "2      [train, wreck, body, train, wreck, im, wreck]  \n",
       "3  [livestream, burned, butter, touched, hot, pla...  \n",
       "4      [another, standuser, detonate, killer, queen]  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdata[['tweet','Normalized_tweet']].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is good for loop for cleaning\n",
    "# sent =[]\n",
    "# for row in Xdata_cleaned:\n",
    "#     sequ = ''\n",
    "#     for word in row:\n",
    "#         sequ = sequ + ' ' + word\n",
    "#     sent.append(sequ)\n",
    "\n",
    "# ptext_cleaned = sent\n",
    "# print(ptext_cleaned[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "word2count = {}\n",
    "for data in ptext_cleaned:\n",
    "    words = data \n",
    "    for word in words:\n",
    "        if word not in word2count.keys():\n",
    "            word2count[word] = 1\n",
    "        else:\n",
    "            word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5830\n"
     ]
    }
   ],
   "source": [
    "print(len(word2count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting best 100 features only\n",
    "import heapq \n",
    "ptext_words = heapq.nlargest(1200, word2count, key=word2count.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting sentences into vectors: Xtrain data\n",
    "x_final = []\n",
    "for data in ptext_cleaned:\n",
    "    vec = []\n",
    "    for word in ptext_words:\n",
    "        if word in data: \n",
    "            vec.append(1)\n",
    "        else:\n",
    "            vec.append(0)\n",
    "    x_final.append(vec)\n",
    "    \n",
    "X_final = np.array(x_final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(X_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1305\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_final, ydata, test_size = 0.33, random_state = 42)\n",
    "\n",
    "import math \n",
    "nn = math.ceil(len(tdata.tweet)*0.7)\n",
    "print(nn)\n",
    "X_train = X_final[:nn]\n",
    "X_test = X_final[nn:]\n",
    "y_train =ydata[:nn]\n",
    "y_test = ydata[nn:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from time import time\n",
    "#from .._base import _BaseClassifier\n",
    "#from .._base import _BaseMultiClass\n",
    "\n",
    "\n",
    "class SoftmaxRegression(object):\n",
    "\n",
    "    \"\"\"Softmax regression classifier.\n",
    "\n",
    "    Parameters\n",
    "    ------------\n",
    "    eta : float (default: 0.01)\n",
    "        Learning rate (between 0.0 and 1.0)\n",
    "    epochs : int (default: 50)\n",
    "        Passes over the training dataset.\n",
    "        Prior to each epoch, the dataset is shuffled\n",
    "        if `minibatches > 1` to prevent cycles in stochastic gradient descent.\n",
    "    l2 : float\n",
    "        Regularization parameter for L2 regularization.\n",
    "        No regularization if l2=0.0.\n",
    "    minibatches : int (default: 1)\n",
    "        The number of minibatches for gradient-based optimization.\n",
    "        If 1: Gradient Descent learning\n",
    "        If len(y): Stochastic Gradient Descent (SGD) online learning\n",
    "        If 1 < minibatches < len(y): SGD Minibatch learning\n",
    "    n_classes : int (default: None)\n",
    "        A positive integer to declare the number of class labels\n",
    "        if not all class labels are present in a partial training set.\n",
    "        Gets the number of class labels automatically if None.\n",
    "    random_seed : int (default: None)\n",
    "        Set random state for shuffling and initializing the weights.\n",
    "\n",
    "    Attributes\n",
    "    -----------\n",
    "    w_ : 2d-array, shape={n_features, 1}\n",
    "      Model weights after fitting.\n",
    "    b_ : 1d-array, shape={1,}\n",
    "      Bias unit after fitting.\n",
    "    cost_ : list\n",
    "        List of floats, the average cross_entropy for each epoch.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, eta=0.01, epochs=50,\n",
    "                 l2=0.0,\n",
    "                 minibatches=1,\n",
    "                 n_classes=None,\n",
    "                 random_seed=None):\n",
    "\n",
    "        self.eta = eta\n",
    "        self.epochs = epochs\n",
    "        self.l2 = l2\n",
    "        self.minibatches = minibatches\n",
    "        self.n_classes = n_classes\n",
    "        self.random_seed = random_seed\n",
    "\n",
    "    def _fit(self, X, y, init_params=True):\n",
    "        if init_params:\n",
    "            if self.n_classes is None:\n",
    "                self.n_classes = np.max(y) + 1\n",
    "            self._n_features = X.shape[1]\n",
    "\n",
    "            self.b_, self.w_ = self._init_params(\n",
    "                weights_shape=(self._n_features, self.n_classes),\n",
    "                bias_shape=(self.n_classes,),\n",
    "                random_seed=self.random_seed)\n",
    "            self.cost_ = []\n",
    "\n",
    "        y_enc = self._one_hot(y=y, n_labels=self.n_classes, dtype=np.float)\n",
    "\n",
    "        for i in range(self.epochs):\n",
    "            for idx in self._yield_minibatches_idx(\n",
    "                    n_batches=self.minibatches,\n",
    "                    data_ary=y,\n",
    "                    shuffle=True):\n",
    "                # givens:\n",
    "                # w_ -> n_feat x n_classes\n",
    "                # b_  -> n_classes\n",
    "\n",
    "                # net_input, softmax and diff -> n_samples x n_classes:\n",
    "                net = self._net_input(X[idx], self.w_, self.b_)\n",
    "                softm = self._softmax(net)\n",
    "                diff = softm - y_enc[idx]\n",
    "                mse = np.mean(diff, axis=0)\n",
    "\n",
    "                # gradient -> n_features x n_classes\n",
    "                grad = np.dot(X[idx].T, diff)\n",
    "                \n",
    "                # update in opp. direction of the cost gradient\n",
    "                self.w_ -= (self.eta * grad +\n",
    "                            self.eta * self.l2 * self.w_)\n",
    "                self.b_ -= (self.eta * np.sum(diff, axis=0))\n",
    "\n",
    "            # compute cost of the whole epoch\n",
    "            net = self._net_input(X, self.w_, self.b_)\n",
    "            softm = self._softmax(net)\n",
    "            cross_ent = self._cross_entropy(output=softm, y_target=y_enc)\n",
    "            cost = self._cost(cross_ent)\n",
    "            self.cost_.append(cost)\n",
    "        return self\n",
    "\n",
    "    def fit(self, X, y, init_params=True):\n",
    "        \"\"\"Learn model from training data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
    "            Training vectors, where n_samples is the number of samples and\n",
    "            n_features is the number of features.\n",
    "        y : array-like, shape = [n_samples]\n",
    "            Target values.\n",
    "        init_params : bool (default: True)\n",
    "            Re-initializes model parametersprior to fitting.\n",
    "            Set False to continue training with weights from\n",
    "            a previous model fitting.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "\n",
    "        \"\"\"\n",
    "        if self.random_seed is not None:\n",
    "            np.random.seed(self.random_seed)\n",
    "        self._fit(X=X, y=y, init_params=init_params)\n",
    "        self._is_fitted = True\n",
    "        return self\n",
    "    \n",
    "    def _predict(self, X):\n",
    "        probas = self.predict_proba(X)\n",
    "        return self._to_classlabels(probas)\n",
    " \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict targets from X.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
    "            Training vectors, where n_samples is the number of samples and\n",
    "            n_features is the number of features.\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        target_values : array-like, shape = [n_samples]\n",
    "          Predicted target values.\n",
    "\n",
    "        \"\"\"\n",
    "        if not self._is_fitted:\n",
    "            raise AttributeError('Model is not fitted, yet.')\n",
    "        return self._predict(X)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict class probabilities of X from the net input.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
    "            Training vectors, where n_samples is the number of samples and\n",
    "            n_features is the number of features.\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        Class probabilties : array-like, shape= [n_samples, n_classes]\n",
    "\n",
    "        \"\"\"\n",
    "        net = self._net_input(X, self.w_, self.b_)\n",
    "        softm = self._softmax(net)\n",
    "        return softm\n",
    "\n",
    "    def _net_input(self, X, W, b):\n",
    "        return (X.dot(W) + b)\n",
    "\n",
    "    def _softmax(self, z):\n",
    "        return (np.exp(z.T) / np.sum(np.exp(z), axis=1)).T\n",
    "\n",
    "    def _cross_entropy(self, output, y_target):\n",
    "        return - np.sum(np.log(output) * (y_target), axis=1)\n",
    "\n",
    "    def _cost(self, cross_entropy):\n",
    "        L2_term = self.l2 * np.sum(self.w_ ** 2)\n",
    "        cross_entropy = cross_entropy + L2_term\n",
    "        return 0.5 * np.mean(cross_entropy)\n",
    "\n",
    "    def _to_classlabels(self, z):\n",
    "        return z.argmax(axis=1)\n",
    "    \n",
    "    def _init_params(self, weights_shape, bias_shape=(1,), dtype='float64',\n",
    "                     scale=0.01, random_seed=None):\n",
    "        \"\"\"Initialize weight coefficients.\"\"\"\n",
    "        if random_seed:\n",
    "            np.random.seed(random_seed)\n",
    "        w = np.random.normal(loc=0.0, scale=scale, size=weights_shape)\n",
    "        b = np.zeros(shape=bias_shape)\n",
    "        return b.astype(dtype), w.astype(dtype)\n",
    "    \n",
    "    def _one_hot(self, y, n_labels, dtype):\n",
    "        \"\"\"Returns a matrix where each sample in y is represented\n",
    "           as a row, and each column represents the class label in\n",
    "           the one-hot encoding scheme.\n",
    "\n",
    "        Example:\n",
    "\n",
    "            y = np.array([0, 1, 2, 3, 4, 2])\n",
    "            mc = _BaseMultiClass()\n",
    "            mc._one_hot(y=y, n_labels=5, dtype='float')\n",
    "\n",
    "            np.array([[1., 0., 0., 0., 0.],\n",
    "                      [0., 1., 0., 0., 0.],\n",
    "                      [0., 0., 1., 0., 0.],\n",
    "                      [0., 0., 0., 1., 0.],\n",
    "                      [0., 0., 0., 0., 1.],\n",
    "                      [0., 0., 1., 0., 0.]])\n",
    "\n",
    "        \"\"\"\n",
    "        mat = np.zeros((len(y), n_labels))\n",
    "        for i, val in enumerate(y):\n",
    "            mat[i, val] = 1\n",
    "        return mat.astype(dtype)    \n",
    "    \n",
    "    def _yield_minibatches_idx(self, n_batches, data_ary, shuffle=True):\n",
    "            indices = np.arange(data_ary.shape[0])\n",
    "\n",
    "            if shuffle:\n",
    "                indices = np.random.permutation(indices)\n",
    "            if n_batches > 1:\n",
    "                remainder = data_ary.shape[0] % n_batches\n",
    "\n",
    "                if remainder:\n",
    "                    minis = np.array_split(indices[:-remainder], n_batches)\n",
    "                    minis[-1] = np.concatenate((minis[-1],\n",
    "                                                indices[-remainder:]),\n",
    "                                               axis=0)\n",
    "                else:\n",
    "                    minis = np.array_split(indices, n_batches)\n",
    "\n",
    "            else:\n",
    "                minis = (indices,)\n",
    "\n",
    "            for idx_batch in minis:\n",
    "                yield idx_batch\n",
    "    \n",
    "    def _shuffle_arrays(self, arrays):\n",
    "        \"\"\"Shuffle arrays in unison.\"\"\"\n",
    "        r = np.random.permutation(len(arrays[0]))\n",
    "        return [ary[r] for ary in arrays]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.SoftmaxRegression at 0x11227d280>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gradient Descent method\n",
    "# from mlxtend.plotting import plot_decision_regions\n",
    "lr = SoftmaxRegression(eta=0.13, epochs=300, minibatches=1, random_seed=0)\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# # ydata = y_train.to_numpy(dtype='int64')\n",
    "# # plot_decision_regions(X_train, ydata, clf=lr)\n",
    "# # plt.title('Softmax Regression - Gradient Descent')\n",
    "# # plt.show()\n",
    "\n",
    "# plt.plot(range(len(lr.cost_)), lr.cost_)\n",
    "# plt.xlabel('Iterations')\n",
    "# plt.ylabel('Cost function')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = lr.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Training Accuracy:  0.9418\n"
     ]
    }
   ],
   "source": [
    "def accuracy(y, y_hat):\n",
    "  return np.mean(y == y_hat)\n",
    "\n",
    "print(f\"Logistic Regression Training Accuracy:  {accuracy(y_train, y_hat):0.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Test Accuracy:  0.5269\n"
     ]
    }
   ],
   "source": [
    "# Gradient Descent method\n",
    "# from mlxtend.plotting import plot_decision_regions\n",
    "lr = SoftmaxRegression(eta=0.1, epochs=100, minibatches=1, random_seed=0)\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# ydata = y_train.to_numpy(dtype='int64')\n",
    "# plot_decision_regions(X_train, ydata, clf=lr)\n",
    "# plt.title('Softmax Regression - Gradient Descent')\n",
    "# plt.show()\n",
    "\n",
    "# plt.plot(range(len(lr.cost_)), lr.cost_)\n",
    "# plt.xlabel('Iterations')\n",
    "# plt.ylabel('Cost function')\n",
    "# plt.show()\n",
    "\n",
    "y_hat = lr.predict(X_test)\n",
    "\n",
    "def accuracy(y, y_hat):\n",
    "  return np.mean(y == y_hat)\n",
    "\n",
    "print(f\"Logistic Regression Test Accuracy:  {accuracy(y_test, y_hat):0.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNNClassifier():\n",
    "    def fit(self,X,y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "    def predict(self, X, K, epsilon=1e-3):\n",
    "        N = len(X)\n",
    "        y_hat = np.zeros(N)\n",
    "        \n",
    "        for i in range(N):\n",
    "            dist2 = np.sum((self.X - X[i])**2,axis=1)\n",
    "            idxt = np.argsort(dist2)[:K]\n",
    "            gamma_k = 1/(np.sqrt(dist2[idxt])+epsilon)\n",
    "            y_hat[i] = np.bincount(self.y[idxt], weights = gamma_k).argmax()\n",
    "            \n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNNClassifier()\n",
    "knn.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = knn.predict(X_train,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Classifier Train Accuracy:  0.9709\n"
     ]
    }
   ],
   "source": [
    "def accuracy(y,y_hat):\n",
    "    return np.mean(y == y_hat)\n",
    "\n",
    "print(f\"KNN Classifier Train Accuracy:  {accuracy(y_train, y_hat):0.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Classifier Test Accuracy:  0.5018\n"
     ]
    }
   ],
   "source": [
    "y_hat = knn.predict(X_test,20)\n",
    "def accuracy(y,y_hat):\n",
    "    return np.mean(y == y_hat)\n",
    "\n",
    "print(f\"KNN Classifier Test Accuracy:  {accuracy(y_test, y_hat):0.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf-IDf model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of words:  5830\n"
     ]
    }
   ],
   "source": [
    "print(\"Bag of words: \",len(word2count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting best 100 features only\n",
    "import heapq \n",
    "freq_words1 = heapq.nlargest(200, word2count, key=word2count.get)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IDF Dictionary (Inverse Document Frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_idfs = {}\n",
    "for word in freq_words1:\n",
    "    doc_count = 0\n",
    "    for data in ptext_cleaned:\n",
    "        if word in data:\n",
    "            doc_count += 1\n",
    "    word_idfs[word] = np.log(len(ptext_cleaned)/(1+doc_count)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_idfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF (Term Frequency) Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_matrix = {}\n",
    "for word in freq_words1:\n",
    "    doc_tf = []\n",
    "    for data in ptext_cleaned:\n",
    "        frequency = 0\n",
    "        for w in data:\n",
    "            if word == w:\n",
    "                frequency += 1\n",
    "        tf_word = frequency/len(ptext_cleaned)\n",
    "        doc_tf.append(tf_word)\n",
    "    tf_matrix[word] = doc_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(tf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Tf-IDf Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_matrix = []\n",
    "for word in tf_matrix.keys():\n",
    "    tfidf = []\n",
    "    for value in tf_matrix[word]:\n",
    "        score = value * word_idfs[word]\n",
    "        tfidf.append(score)\n",
    "    tfidf_matrix.append(tfidf)\n",
    "    \n",
    "# tfidf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1863, 200)\n"
     ]
    }
   ],
   "source": [
    "Xdata = np.asarray(tfidf_matrix)\n",
    "Xdata = np.transpose(Xdata)\n",
    "print(Xdata.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1305\n"
     ]
    }
   ],
   "source": [
    "import math \n",
    "nn = math.ceil(len(tdata.tweet)*0.7)\n",
    "print(nn)\n",
    "X_train1 = Xdata[:nn]\n",
    "X_test1 = Xdata[nn:]\n",
    "y_train1 =ydata[:nn]\n",
    "y_test1 = ydata[nn:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf-IDf Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.SoftmaxRegression at 0x111539f40>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gradient Descent method\n",
    "# from mlxtend.plotting import plot_decision_regions\n",
    "lr = SoftmaxRegression(eta=0.11, epochs=100, minibatches=1, random_seed=0)\n",
    "lr.fit(X_train1, y_train1)\n",
    "\n",
    "# ydata = y_train.to_numpy(dtype='int64')\n",
    "# plot_decision_regions(X_train, ydata, clf=lr)\n",
    "# plt.title('Softmax Regression - Gradient Descent')\n",
    "# plt.show()\n",
    "\n",
    "# plt.plot(range(len(lr.cost_)), lr.cost_)\n",
    "# plt.xlabel('Iterations')\n",
    "# plt.ylabel('Cost function')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = lr.predict(X_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tf-IDf Training Accuracy:  0.6054\n"
     ]
    }
   ],
   "source": [
    "def accuracy(y, y_hat):\n",
    "  return np.mean(y == y_hat)\n",
    "print(f\"Tf-IDf Training Accuracy:  {accuracy(y_train1, y_hat):0.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tf-IDf Test Accuracy:  0.5018\n"
     ]
    }
   ],
   "source": [
    "y_hat = lr.predict(X_test1)\n",
    "\n",
    "def accuracy(y, y_hat):\n",
    "  return np.mean(y == y_hat)\n",
    "print(f\"Tf-IDf Test Accuracy:  {accuracy(y_test1, y_hat):0.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
